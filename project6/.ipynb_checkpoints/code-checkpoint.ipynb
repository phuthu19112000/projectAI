{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face Recognition\n",
    "\n",
    "Trong bài tập này, ta sẽ xây dựng một hệ thống nhận dạng khuôn mặt. Nhiều ý tưởng được trình bày ở đây là từ  [FaceNet](https://arxiv.org/pdf/1503.03832.pdf), va [DeepFace](https://research.fb.com/wp-content/uploads/2016/11/deepface-closing-the-gap-to-human-level-performance-in-face-verification.pdf). \n",
    "\n",
    "Các vấn đề nhận dạng khuôn mặt thường chia thành hai loại:\n",
    "\n",
    "- **Face Verification** - \"đây có phải là người đang yêu cầu không?\". Ví dụ, tại một số sân bay, ta có thể thông qua trạm hải quan bằng cách để hệ thống quét hộ chiếu và sau đó xác minh rằng bạn (người mang hộ chiếu) có phải là người chính xác hay không. Điện thoại di động mở khóa bằng khuôn mặt cũng đang sử dụng xác minh khuôn mặt. Đây là một bài toán đối sánh 1:1\n",
    "\n",
    "- **Face Recognition** - \"người này là ai?\". Đây là một bài toán đối sánh 1: K.\n",
    "\n",
    "FaceNet học một mạng nơ-ron mã hóa hình ảnh khuôn mặt thành một vectơ gồm 128 units. Bằng cách so sánh hai vectơ như vậy, ta có thể xác định xem hai bức ảnh có phải của cùng một người hay không.\n",
    "    \n",
    "**Trong bài vấn đề này ta sẽ đưa ra các bước thực hiện sau:**\n",
    "- Thực hiện triplet loss function\n",
    "- Sử dụng mô hình được đào tạo trước để ánh xạ hình ảnh khuôn mặt thành các encoding 128 chiều\n",
    "- Sử dụng các mã hóa này để thực hiện xác minh khuôn mặt và nhận dạng khuôn mặt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load packages\n",
    "Hãy load các thư viện cần thiết. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "threshold must be non-NAN, try sys.maxsize for untruncated representation",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-7bae747a1a03>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'autoreload'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'2'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_printoptions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\core\\arrayprint.py\u001b[0m in \u001b[0;36mset_printoptions\u001b[1;34m(precision, threshold, edgeitems, linewidth, suppress, nanstr, infstr, formatter, sign, floatmode, **kwarg)\u001b[0m\n\u001b[0;32m    257\u001b[0m     opt = _make_options_dict(precision, threshold, edgeitems, linewidth,\n\u001b[0;32m    258\u001b[0m                              \u001b[0msuppress\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnanstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msign\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformatter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 259\u001b[1;33m                              floatmode, legacy)\n\u001b[0m\u001b[0;32m    260\u001b[0m     \u001b[1;31m# formatter is always reset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m     \u001b[0mopt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'formatter'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mformatter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\core\\arrayprint.py\u001b[0m in \u001b[0;36m_make_options_dict\u001b[1;34m(precision, threshold, edgeitems, linewidth, suppress, nanstr, infstr, sign, formatter, floatmode, legacy)\u001b[0m\n\u001b[0;32m     93\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"threshold must be numeric\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 95\u001b[1;33m             raise ValueError(\"threshold must be non-NAN, try \"\n\u001b[0m\u001b[0;32m     96\u001b[0m                              \"sys.maxsize for untruncated representation\")\n\u001b[0;32m     97\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: threshold must be non-NAN, try sys.maxsize for untruncated representation"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, ZeroPadding2D, Activation, Input, concatenate\n",
    "from keras.models import Model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.pooling import MaxPooling2D, AveragePooling2D\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.layers.core import Lambda, Flatten, Dense\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.engine.topology import Layer\n",
    "from keras import backend as K\n",
    "K.set_image_data_format('channels_first')\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from fr_utils import *\n",
    "from inception_blocks_v2 import *\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.set_printoptions(threshold=np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 - Xác minh khuôn mặt\n",
    "\n",
    "Trong Xác minh khuôn mặt, bạn được cung cấp hai hình ảnh và bạn phải xác định xem chúng có phải của cùng một người hay không. Cách đơn giản nhất để làm điều này là so sánh hai hình ảnh theo từng pixel. Nếu khoảng cách giữa hai hình ảnh nhỏ hơn một ngưỡng đã chọn, đó có thể là cùng một người!\n",
    "\n",
    "<img src=\"images/pixel_comparison.png\" style=\"width:380px;height:150px;\">\n",
    "<caption><center> <u> <font color='purple'> Figure 1 </u></center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Tất nhiên, thuật toán này hoạt động thực sự kém, vì các giá trị pixel thay đổi đáng kể do sự thay đổi về ánh sáng, hướng khuôn mặt của người đó, thậm chí cả những thay đổi nhỏ ở vị trí đầu, v.v.\n",
    "* Ta sẽ thấy rằng thay vì sử dụng hình ảnh raw, ta có thể học cách mã hóa, $f(img)$.  \n",
    "* Bằng cách sử dụng encoding cho mỗi hình ảnh, phép so sánh element-wise sẽ đưa ra dự đoán chính xác hơn về việc liệu hai hình ảnh có phải của cùng một người hay không."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Mã hóa hình ảnh khuôn mặt thành một vector 128 chiều\n",
    "\n",
    "### 1.1 - Sử dụng ConvNet để tính toán các mã hóa\n",
    "\n",
    "Mô hình FaceNet cần rất nhiều dữ liệu và thời gian đào tạo lâu dài. Vì vậy, một cách người ta thường làm trong học sâu là tải trọng số và load model đã được đào tạo trước. Kiến trúc mạng tuân theo mô hình tại Figure 3 trong paper [Szegedy *et al.*](https://arxiv.org/abs/1409.4842). Kiến trúc model ta có thể xem trong file `inception_blocks_v2.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Những điều quan trọng bạn cần biết là:\n",
    "\n",
    "- Mạng này sử dụng hình ảnh RGB 96x96 chiều làm đầu vào. Cụ thể, nhập hình ảnh khuôn mặt (hoặc một batch hình ảnh khuôn mặt $m$) dưới dạng một khối $(m, n_C, n_H, n_W) = (m, 3, 96, 96)$ \n",
    "- Nó xuất ra một ma trận có dạng $(m, 128)$ mã hóa từng hình ảnh khuôn mặt đầu vào thành một vectơ 128 chiều\n",
    "\n",
    "Mã sau tạo model cho hình ảnh khuôn mặt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FRmodel = faceRecoModel(input_shape=(3, 96, 96))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Params: 3743280\n"
     ]
    }
   ],
   "source": [
    "print(\"Total Params:\", FRmodel.count_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bằng cách sử dụng lớp kết nối đầy đủ 128 nơ-ron làm lớp cuối cùng , mô hình đảm bảo rằng đầu ra là một vectơ mã hóa có kích thước 128. Sau đó, ta sử dụng các mã hóa để so sánh hai hình ảnh khuôn mặt như sau:\n",
    "\n",
    "<img src=\"images/distance_kiank.png\" style=\"width:680px;height:250px;\">\n",
    "\n",
    "Vì vậy, một encoding tốt nếu:\n",
    "- Các encoding của hai hình ảnh của cùng một người khá giống nhau.\n",
    "- Các encoding của hai hình ảnh của những người khác nhau rất khác nhau.\n",
    "\n",
    "Triplet loss function  sẽ là hàm loss function và cố gắng \"đẩy\" mã hóa của hai hình ảnh của cùng một người (Anchor và Positive) gần nhau hơn, đồng thời \"kéo\" các mã hóa của hai hình ảnh của những người khác nhau (Anchor, Negative) xa nhau hơn. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 -Triplet Loss function\n",
    "\n",
    "Đối với hình ảnh $x$, chúng ta ký hiệu mã hóa của nó là $f(x)$, trong đó $f$ là hàm được tính toán bởi mạng nơ-ron.\n",
    "\n",
    "<img src=\"images/f_x.png\" style=\"width:380px;height:150px;\">\n",
    "\n",
    "<!--\n",
    "We will also add a normalization step at the end of our model so that $\\mid \\mid f(x) \\mid \\mid_2 = 1$ (means the vector of encoding should be of norm 1).\n",
    "!-->\n",
    "\n",
    "Training sẽ sử dụng triplets of images $(A, P, N)$:  \n",
    "\n",
    "- A là \"Anchor\" image--bức ảnh của một người \n",
    "- P là \"Positive\" image--bức ảnh của cùng một người giống A\n",
    "- N là \"Negative\" image--bức ảnh của một người khác A\n",
    "\n",
    "Bộ ba ảnh này được chọn từ tập dữ liệu đào tạo của chúng ta. Chúng ta sẽ viết $ (A ^ {(i)}, P ^ {(i)}, N ^ {(i)}) $ để biểu thị ví dụ đào tạo $ i $ -th.\n",
    "\n",
    "Ta muốn chắc chắn rằng hình ảnh  $A^{(i)}$  gần với ảnh $P^{(i)}$ hơn ảnh $N^{(i)}$):\n",
    "\n",
    "$$\\mid \\mid f(A^{(i)}) - f(P^{(i)}) \\mid \\mid_2^2 + \\alpha < \\mid \\mid f(A^{(i)}) - f(N^{(i)}) \\mid \\mid_2^2$$\n",
    "\n",
    "Do đố ta sẽ tối ưu hóa hàm \"triplet cost\":\n",
    "\n",
    "$$\\mathcal{J} = \\sum^{m}_{i=1} \\large[ \\small \\underbrace{\\mid \\mid f(A^{(i)}) - f(P^{(i)}) \\mid \\mid_2^2}_\\text{(1)} - \\underbrace{\\mid \\mid f(A^{(i)}) - f(N^{(i)}) \\mid \\mid_2^2}_\\text{(2)} + \\alpha \\large ] \\small_+ \\tag{3}$$\n",
    "\n",
    "Ta sử dụng kí hiệu \"$[z]_+$\" có nghĩa là $max(z,0)$.  \n",
    "\n",
    "Notes:\n",
    "- ta muốn (1) nhỏ.\n",
    "- Và muốn (2) là khoảng cách bình phương giữa \"A\" và \"N\" tương đối lớn.\n",
    "- $\\alpha$ được gọi là ngưỡng. Nó là một hyperparameter mà ta chọn theo cách thủ công. Chúng ta sẽ sử dụng $\\alpha = 0.2$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def triplet_loss(y_true, y_pred, alpha = 0.2):\n",
    "    \"\"\"\n",
    "    Thực hiện triplet_loss như được xác định bởi công thức(3)\n",
    "    \n",
    "    Arguments:\n",
    "    y_true -- true labels\n",
    "    y_pred -- danh sách python chứa ba đối tượng:\n",
    "            anchor -- encodings cho hình ảnh anchor có shape (None, 128)\n",
    "            positive -- encodings cho hình ảnh positive có shape (None, 128)\n",
    "            negative -- encodings cho hình ảnh negative có shape (None, 128)\n",
    "    \n",
    "    Returns:\n",
    "    loss -- số thực\n",
    "    \"\"\"\n",
    "    \n",
    "    anchor, positive, negative = y_pred[0], y_pred[1], y_pred[2]\n",
    "    \n",
    "    # Step 1: Tính toán khoảng cách (encoding) giữa A và P\n",
    "    pos_dist = tf.reduce_sum(tf.square(tf.subtract(anchor,positive)),axis=-1)\n",
    "    # Step 2: Tính toán khoảng cách (encoding) giữa A và N\n",
    "    neg_dist = tf.reduce_sum(tf.square(tf.subtract(anchor,negative)),axis=-1)\n",
    "    # Step 3: trừ hai khoảng cách trước đó và cộng với alpha.\n",
    "    basic_loss = tf.add((tf.subtract(pos_dist,neg_dist)),alpha)\n",
    "    # Step 4: Lấy max(basic_loss,0)\n",
    "    loss = tf.reduce_sum(tf.maximum(basic_loss,0.0))\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Load model được đào tạo trước\n",
    "\n",
    "FaceNet được đào tạo bằng cách tối ưu hóa triplet loss function. Nhưng vì quá trình đào tạo đòi hỏi nhiều dữ liệu và tính toán nhiều đồng nghĩa với máy tính hiện đại để train nên ta sẽ không đào tạo lại từ đầu. Thay vào đó,load một mô hình đã được đào tạo trước đó."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FRmodel.compile(optimizer = 'adam', loss = triplet_loss, metrics = ['accuracy'])\n",
    "load_weights_from_FaceNet(FRmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ví dụ về khoảng cách giữa các encoding:\n",
    "\n",
    "<img src=\"images/distance_matrix.png\" style=\"width:380px;height:200px;\">\n",
    "<br>\n",
    "\n",
    "Tiếp theo ta hãy sử dụng mô hình này để thực hiện xác minh khuôn mặt và nhận dạng khuôn mặt!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Áp dụng model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Giả sử ta đang xây dựng một hệ thống cho một tòa nhà văn phòng và người quản lý tòa nhà muốn cung hệ thống nhận dạng khuôn mặt để cho phép nhân viên ra vào tòa nhà.\n",
    "\n",
    "Ta muốn xây dựng hệ thống **Xác minh khuôn mặt** cho phép truy cập vào danh sách những nhân viên có trong database của tòa nhà. Để được nhận vào, mỗi người phải quẹt thẻ ID (thẻ căn cước) để nhận dạng mình ở cửa ra vào. Sau đó, hệ thống xác minh khuôn mặt sẽ kiểm tra xem họ có phải là nhân viên trong tòa nhà hay không."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 - Xác minh khuôn mặt\n",
    "\n",
    "Hãy xây dựng một cơ sở dữ liệu chứa một vectơ mã hóa cho mỗi người được phép vào văn phòng. Để tạo encoding, chúng ta sử dụng ```img_to_encoding(image_path, model)```\n",
    "\n",
    "Mã sau sẽ xây dựng database (được biểu diễn dưới dạng python dict). Database này ánh xạ  mỗi người thành encoding 128 chiều trên khuôn mặt của họ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'img_to_encoding' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-3909d017623e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdatabase\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdatabase\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"danielle\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimg_to_encoding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"images/danielle.png\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFRmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mdatabase\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"younes\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimg_to_encoding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"images/younes.jpg\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFRmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdatabase\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"tian\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimg_to_encoding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"images/tian.jpg\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFRmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdatabase\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"andrew\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimg_to_encoding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"images/andrew.jpg\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFRmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'img_to_encoding' is not defined"
     ]
    }
   ],
   "source": [
    "database = {}\n",
    "database[\"danielle\"] = img_to_encoding(\"images/danielle.png\", FRmodel)\n",
    "database[\"younes\"] = img_to_encoding(\"images/younes.jpg\", FRmodel)\n",
    "database[\"tian\"] = img_to_encoding(\"images/tian.jpg\", FRmodel)\n",
    "database[\"andrew\"] = img_to_encoding(\"images/andrew.jpg\", FRmodel)\n",
    "database[\"Hieu Le\"] = img_to_encoding(\"images/hieu.jpg\", FRmodel)\n",
    "database[\"kian\"] = img_to_encoding(\"images/kian.jpg\", FRmodel)\n",
    "database[\"dan\"] = img_to_encoding(\"images/dan.jpg\", FRmodel)\n",
    "database[\"sebastiano\"] = img_to_encoding(\"images/sebastiano.jpg\", FRmodel)\n",
    "database[\"bertrand\"] = img_to_encoding(\"images/bertrand.jpg\", FRmodel)\n",
    "database[\"kevin\"] = img_to_encoding(\"images/kevin.jpg\", FRmodel)\n",
    "database[\"felix\"] = img_to_encoding(\"images/felix.jpg\", FRmodel)\n",
    "database[\"benoit\"] = img_to_encoding(\"images/benoit.jpg\", FRmodel)\n",
    "database[\"arnaud\"] = img_to_encoding(\"images/arnaud.jpg\", FRmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bây giờ, khi ai đó xuất hiện ở cửa trước và quẹt thẻ ID của họ (do đó cung cấp cho ta tên của họ), ta có thể tra cứu encoding của họ trong database và sử dụng nó để kiểm tra xem người đứng ở cửa trước có khớp với tên trên không ID.\n",
    "\n",
    "Mã sau thực hiện từng bước như sau:\n",
    "\n",
    "1. Tính toán mã hóa của hình ảnh từ `image_path`.\n",
    "2. Tính toán khoảng cách giữa encoding này và encoding hình ảnh nhận dạng được lưu trữ trong database.\n",
    "3. Mở cửa nếu khoảng cách nhỏ hơn ngưỡng đặt ra ở đây là 0,7, nếu không thì không được mở.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def verify(image_path, identity, database, model):\n",
    "    \"\"\"\n",
    "    Hàm xác minh xem người trên hình ảnh \"image_path\" có phải là \"identity\" hay không.\n",
    "    \n",
    "    Arguments:\n",
    "    image_path -- hình ảnh cần xác minh\n",
    "    identity -- tên của người bạn muốn xác minh danh tính. Phải là nhân viên làm việc văn phòng.\n",
    "    database -- từ điển python ánh xạ tên của tên người được phép với mã hóa encoding của họ.\n",
    "    model -- model ta đã load\n",
    "    \n",
    "    Returns:\n",
    "    dist -- khoảng cách giữa image_path và hình ảnh của \"identity\" trong database.\n",
    "    door_open -- True nếu cùng một người, False nếu otherwise\n",
    "    \"\"\"\n",
    "     \n",
    "    # Step 1: Tính toán encoding cho hình ảnh.\n",
    "    encoding = img_to_encoding(image_path,model)\n",
    "    \n",
    "    # Step 2: Tính toán khoảng cách\n",
    "    dist = np.linalg.norm(encoding - database[identity])\n",
    "    \n",
    "    # Step 3: Mở cửa nếu dist < 0.7, else không mở cửa\n",
    "    if dist<0.7:\n",
    "        print(\"Đây là \" + str(identity) + \", Chào mừng bạn\")\n",
    "        door_open = True\n",
    "    else:\n",
    "        print(\"Đây không phải là \" + str(identity) + \", Bạn không phải nhân viên công ty\")\n",
    "        door_open = False\n",
    "        \n",
    "    return dist, door_open"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Younes is trying to enter the office and the camera takes a picture of him (\"images/camera_0.jpg\"). Let's run your verification algorithm on this picture:\n",
    "\n",
    "<img src=\"images/camera_0.jpg\" style=\"width:100px;height:100px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's younes, welcome in!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.65939289, True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verify(\"images/camera_0.jpg\", \"younes\", database, FRmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **It's younes, welcome in!**\n",
    "        </td>\n",
    "        <td>\n",
    "           (0.65939283, True)\n",
    "        </td>\n",
    "    </tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Benoit, who does not work in the office, stole Kian's ID card and tried to enter the office. The camera took a picture of Benoit (\"images/camera_2.jpg). Let's run the verification algorithm to check if benoit can enter.\n",
    "<img src=\"images/camera_2.jpg\" style=\"width:100px;height:100px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's not kian, please go away\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.86224014, False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verify(\"images/camera_2.jpg\", \"kian\", database, FRmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **It's not kian, please go away**\n",
    "        </td>\n",
    "        <td>\n",
    "           (0.86224014, False)\n",
    "        </td>\n",
    "    </tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 - Face Recognition\n",
    "\n",
    "Your face verification system is mostly working well. But since Kian got his ID card stolen, when he came back to the office the next day and couldn't get in! \n",
    "\n",
    "To solve this, you'd like to change your face verification system to a face recognition system. This way, no one has to carry an ID card anymore. An authorized person can just walk up to the building, and the door will unlock for them! \n",
    "\n",
    "You'll implement a face recognition system that takes as input an image, and figures out if it is one of the authorized persons (and if so, who). Unlike the previous face verification system, we will no longer get a person's name as one of the inputs. \n",
    "\n",
    "**Exercise**: Implement `who_is_it()`. You will have to go through the following steps:\n",
    "1. Compute the target encoding of the image from image_path\n",
    "2. Find the encoding from the database that has smallest distance with the target encoding. \n",
    "    - Initialize the `min_dist` variable to a large enough number (100). It will help you keep track of what is the closest encoding to the input's encoding.\n",
    "    - Loop over the database dictionary's names and encodings. To loop use `for (name, db_enc) in database.items()`.\n",
    "        - Compute the L2 distance between the target \"encoding\" and the current \"encoding\" from the database.\n",
    "        - If this distance is less than the min_dist, then set `min_dist` to `dist`, and `identity` to `name`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: who_is_it\n",
    "\n",
    "def who_is_it(image_path, database, model):\n",
    "    \"\"\"\n",
    "    Implements face recognition for the office by finding who is the person on the image_path image.\n",
    "    \n",
    "    Arguments:\n",
    "    image_path -- path to an image\n",
    "    database -- database containing image encodings along with the name of the person on the image\n",
    "    model -- your Inception model instance in Keras\n",
    "    \n",
    "    Returns:\n",
    "    min_dist -- the minimum distance between image_path encoding and the encodings from the database\n",
    "    identity -- string, the name prediction for the person on image_path\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### \n",
    "    \n",
    "    ## Step 1: Compute the target \"encoding\" for the image. Use img_to_encoding() see example above. ## (≈ 1 line)\n",
    "    encoding = img_to_encoding(image_path,model)\n",
    "    \n",
    "    ## Step 2: Find the closest encoding ##\n",
    "    \n",
    "    # Initialize \"min_dist\" to a large value, say 100 (≈1 line)\n",
    "    min_dist = 100\n",
    "    \n",
    "    # Loop over the database dictionary's names and encodings.\n",
    "    for (name, db_enc) in database.items():\n",
    "        \n",
    "        # Compute L2 distance between the target \"encoding\" and the current db_enc from the database. (≈ 1 line)\n",
    "        dist = np.linalg.norm(encoding - db_enc)\n",
    "\n",
    "        # If this distance is less than the min_dist, then set min_dist to dist, and identity to name. (≈ 3 lines)\n",
    "        if dist<min_dist:\n",
    "            min_dist = dist\n",
    "            identity = name\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    if min_dist > 0.7:\n",
    "        print(\"Not in the database.\")\n",
    "    else:\n",
    "        print (\"it's \" + str(identity) + \", the distance is \" + str(min_dist))\n",
    "        \n",
    "    return min_dist, identity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Younes is at the front-door and the camera takes a picture of him (\"images/camera_0.jpg\"). Let's see if your who_it_is() algorithm identifies Younes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it's younes, the distance is 0.659393\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.65939289, 'younes')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "who_is_it(\"images/camera_0.jpg\", database, FRmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **it's younes, the distance is 0.659393**\n",
    "        </td>\n",
    "        <td>\n",
    "           (0.65939283, 'younes')\n",
    "        </td>\n",
    "    </tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can change \"`camera_0.jpg`\" (picture of younes) to \"`camera_1.jpg`\" (picture of bertrand) and see the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Congratulations!\n",
    "\n",
    "* Your face recognition system is working well! It only lets in authorized persons, and people don't need to carry an ID card around anymore! \n",
    "* You've now seen how a state-of-the-art face recognition system works.\n",
    "\n",
    "#### Ways to improve your facial recognition model\n",
    "Although we won't implement it here, here are some ways to further improve the algorithm:\n",
    "- Put more images of each person (under different lighting conditions, taken on different days, etc.) into the database. Then given a new image, compare the new face to multiple pictures of the person. This would increase accuracy.\n",
    "- Crop the images to just contain the face, and less of the \"border\" region around the face. This preprocessing removes some of the irrelevant pixels around the face, and also makes the algorithm more robust.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key points to remember\n",
    "- Face verification solves an easier 1:1 matching problem; face recognition addresses a harder 1:K matching problem. \n",
    "- The triplet loss is an effective loss function for training a neural network to learn an encoding of a face image.\n",
    "- The same encoding can be used for verification and recognition. Measuring distances between two images' encodings allows you to determine whether they are pictures of the same person. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congrats on finishing this assignment! \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References:\n",
    "\n",
    "- Florian Schroff, Dmitry Kalenichenko, James Philbin (2015). [FaceNet: A Unified Embedding for Face Recognition and Clustering](https://arxiv.org/pdf/1503.03832.pdf)\n",
    "- Yaniv Taigman, Ming Yang, Marc'Aurelio Ranzato, Lior Wolf (2014). [DeepFace: Closing the gap to human-level performance in face verification](https://research.fb.com/wp-content/uploads/2016/11/deepface-closing-the-gap-to-human-level-performance-in-face-verification.pdf) \n",
    "- The pretrained model we use is inspired by Victor Sy Wang's implementation and was loaded using his code: https://github.com/iwantooxxoox/Keras-OpenFace.\n",
    "- Our implementation also took a lot of inspiration from the official FaceNet github repository: https://github.com/davidsandberg/facenet \n"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "convolutional-neural-networks",
   "graded_item_id": "IaknP",
   "launcher_item_id": "5UMr4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
