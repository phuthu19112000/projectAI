{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There are two main approaches to finding feature points and their correspondences. \n",
    "    - The first is to find features in one image that can be accurately tracked using a local search technique, such as correlation or least squares\n",
    "    - The second is to independently detect features in all the images under consideration and then match features based on their local appearance\n",
    "\n",
    "- We split the keypoint detection and matching pipeline into four separate stages\n",
    "\n",
    "    - During the feature detection (extraction) stage (Section 7.1.1), each image is searched for locations that are likely to match well in other images.\n",
    "        - Forstnerâ€“Harris, DoG (difference of Gaussian)\n",
    "        - Adaptive non-maximal suppression (ANMS)\n",
    "        - Scale invariance\n",
    "            - This kind of approach is suitable when the images being matched do not undergo large scale changes\n",
    "        - Rotational invariance and orientation estimation\n",
    "        - Affine invariance\n",
    "            - Maximally stable extremal region (MSER)\n",
    "\n",
    "    - In the feature description stage (Section 7.1.2), each region around detected keypoint locations is converted into a more compact and stable (invariant) descriptor that can be matched against other descriptors. (GLOH performed best, followed closely by SIFT)\n",
    "        - Bias and gain normalization (MOPS)\n",
    "        - Scale invariant feature transform (SIFT)\n",
    "        - PCA-SIFT\n",
    "        - RootSIFT\n",
    "        - Gradient location-orientation histogram (GLOH)\n",
    "        - Since 2015 or so, most of the new feature descriptors are constructed using deep learning techniques:\n",
    "            - (LIFT, TFeat, HPatches, L2-Net, HardNet, Geodesc, SOSNet, Key.Net) operate on patches, much like the classical SIFT approach.  They hence require an initial local feature detector to determine the center of the patch and use a predetermined patch size when constructing the input to the network\n",
    "            - Approaches such as DELF, SuperPoint, D2-Net, ContextDesc, R2D2, ASLFeat and CAPS use the entire image as the input to the descriptor computation\n",
    "\n",
    "    - The feature matching stage (Sections 7.1.3 and 7.1.4) efficiently searches for likely matching candidates in other images.\n",
    "        - Matching startegy and error rates\n",
    "        - Efficient matching\n",
    "        - Feature match verification and densification\n",
    "\n",
    "\n",
    "    - The feature tracking stage (Section 7.1.5) is an alternative to the third stage that only searches a small neighborhood around each detected feature and is therefore more suitable for video processing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "video_analyzer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.15 (main, Nov  4 2022, 16:35:55) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "887649faabf951220c588ab34588d0de68cb76fdd246ff11f0af3675fda70d3b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
