{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised learning\n",
    "\n",
    "#### 5.1.1 Nearest neighbors\n",
    "- When k is too small, the classifier acts in a very random way, i.e., it is overfitting to the training data. As k gets larger, the classifier underfits (over-smooths) the data, resulting in\n",
    "the shrinkage of the two smaller regions. \n",
    "\n",
    "- The optimal number of nearest neighbors to use k is a hyperparameter for this algorithm\n",
    "\n",
    "- [Fast Library for Approximate Nearest Neighbors (FLANN)](https://github.com/flann-lib/flann), which collects a number of previously developed algorithms and is incorporated as part of OpenCV\n",
    "\n",
    "- More recently (2021),  developed the GPU-enabled [Faiss library](https://github.com/facebookresearch/faiss) for scaling similarity search to billions of vectors\n",
    "\n",
    "#### 5.1.3 Logistic regression\n",
    "- For the binary classification task, let ti = [0,1] be the class label for each training\n",
    "sample xi and pi = p(C0|x) be the estimated likelihood predicted by for a given weight and bias (w; b). We can maximize the likelihood of the correct labels being predicted by minimizing the negative log likelihood, i.e., the cross-entropy loss or error function\n",
    "\n",
    "- To determine the best set of weights and biases, we can use gradient descent\n",
    "\n",
    "- Logistic regression does have some limitations, which is why it is often used for only the simplest classification tasks\n",
    "    - If the classes in feature space are not linearly separable, using simple projections onto weight vectors may not produce adequate decision surfaces. In this case, kernel methods, which measure the distances between new (test) feature vectors and select training examples, can often provide good solutions.\n",
    "    - Another problem with logistic regression is that if the classes actually are separable, there can be more than a single unique separating plane\n",
    "\n",
    "#### 5.1.4 Support vector machines\n",
    "- how can we choose the best decision surface, keeping in mind that we only have a limited number of training examples. The answer to this problem is to use maximum margin classifiers. The\n",
    "maximum margin classifier provides our best bet for correctly classifying as many of these\n",
    "unseen examples as possible\n",
    "\n",
    "- What happens if the two classes are not linearly separable, and in fact require a complex\n",
    "curved surface to correctly classify samples. In this case, we can replace linear regression with kernel regression \n",
    "\n",
    "- Support vector machines can also be applied to overlapping (mixed) class distributions\n",
    "\n",
    "#### 5.1.5 Decision trees and forests\n",
    "- decision trees perform a sequence of simpler operations, often just looking at individual feature elements before deciding which element to look at next\n",
    "\n",
    "- A random forest is created by building a set of decision trees, each of which makes\n",
    "slightly different decisions. At test time, a new sample is classified by each of\n",
    "the trees in the random forest, and the class distributions at the final leaf nodes are averaged\n",
    "to provide an answer that is more accurate than could be obtained with a single tree\n",
    "\n",
    "- Random forests have several design parameters:\n",
    "    - the depth of each tree D\n",
    "    - the number of trees T \n",
    "    - the number of samples examined at node construction time ρ"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised learning\n",
    "\n",
    "- Examples of Unsupervised learning in computer vision include image segmentation and face and body recognition and reconstruction\n",
    "\n",
    "#### 5.2.1 Clustering\n",
    "- In data clustering, algorithms can link clusters together based on the distance between\n",
    "their closest points (single-link clustering), their farthest points (complete-link clustering)\n",
    "\n",
    "- Mean-shift and mode finding techniques, such as k-means and mixtures of Gaussians, model the feature vectors associated with each pixel (e.g., color and position) as samples from an unknown probability density function and then try to find clusters (modes) in this distribution.\n",
    "\n",
    "#### 5.2.2 K-means and Gaussians mixture models\n",
    "- K-means\n",
    "    - the algorithm is given the number of clusters k it is supposed to find and is initialized by randomly sampling k centers from the input feature vectors.\n",
    "    - It then iteratively updates the cluster center location based on the samples that are closest to each center \n",
    "\n",
    "- Gaussians mixture:\n",
    "    - each cluster center is augmented by a covariance matrix whose values are re-estimated from the corresponding samples \n",
    "    - Instead of using nearest neighbors to associate input samples with cluster centers, a Mahalanobis distance\n",
    "\n",
    "#### 5.2.3 Principal component analysis\n",
    "- PCA was originally used in computer vision for modeling faces, i.e., eigenfaces, initially for gray-scale images, and then for 3D models and active appearance models\n",
    "\n",
    "#### 5.2.4 Manifold learning\n",
    "- In many cases, the data we are analyzing does not reside in a globally linear subspace, but\n",
    "does live on a lower-dimensional manifold. In this case, non-linear dimensionality reduction\n",
    "can be used \n",
    "\n",
    "- Since these systems extract lower-dimensional manifolds in a higher-dimensional space, they are also known as manifold learning techniques\n",
    "\n",
    "- In addition to dimensionality reduction, which can be useful for regularizing data and\n",
    "accelerating similarity search, manifold learning algorithms can be used for visualizing input data distributions or neural network layer activations\n",
    "\n",
    "#### 5.2.5 Semi-supervised learning\n",
    "- if only a small number of examples are labeled with the correct class, we can still imagine extending these labels (inductively) to nearby samples and therefore not only labeling all of the data, but also constructing appropriate decision surfaces for future inputs, This area of study is called semi-supervised learning\n",
    "\n",
    "- In general, it comes in two varieties\n",
    "    - transductive learning: the goal is to classify all of the unlabeled inputs that are given as one batch at the same time as the labeled examples\n",
    "\n",
    "    - inductive learning: we train a machine learning system that will classify all future inputs\n",
    "\n",
    "- Semi-supervised learning is a subset of the larger class of weakly supervised learning problems"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep neural networks\n",
    "\n",
    "#### 5.3.1 Weights and layers\n",
    "\n",
    "- Deep neural networks (DNNs) are feedforward computation graphs composed of thousands of simple interconnected “neurons” (units), which, much like logistic regression, perform weighted sums of their inputs, followed by a non-linear activation function re-mapping\n",
    "\n",
    "- A layer in which a full (dense) weight matrix is used for the linear combination is called\n",
    "a fully connected (FC) layer\n",
    "\n",
    "- A network that consists only of fully connected (and no convolutional) layers is now often called a multi-layer perceptron (MLP).\n",
    "\n",
    "#### 5.3.2 Activation functions\n",
    "\n",
    "- For the final layer in networks used for classification, the softmax function is normally used to convert from real-valued activations to class likelihoods\n",
    "\n",
    "#### 5.3.3 Regularization and normalization\n",
    "- Regularization and weight decay\n",
    "\n",
    "    - As the weights are being optimized inside a neural network, these terms make the weights smaller, so this kind of regularization is also known as weight decay\n",
    "\n",
    "- Dataset augmentation: Another powerful technique to reduce over-fitting is to add more training samples by perturbing the inputs and/or outputs of the samples that have already been collected.\n",
    "\n",
    "- Dropout\n",
    "    - Dropout is a regularization technique where at each mini-batch during training, some percentage p (say 50%) of the units in each layer are clamped to zero\n",
    "    - Randomly setting units to zero injects noise into the training process which can help reduce overfitting and improve generalization.\n",
    "\n",
    "- Batch normalization\n",
    "    - The idea behind batch normalization is to re-scale (and recenter) the activations at a given unit so that they have unit variance and zero mean. After batch normalization, activations now have zero mean and unit variance.\n",
    "\n",
    "    - Since the publication of the seminal paper by Ioffe and Szegedy (2015), a number of  variants have been developed:\n",
    "\n",
    "        ![](./images/i5.png)\n",
    "        - all the activations in a layer, which is called layer normalization\n",
    "        - all the activations in a given convolutional output channel (see Section 5.4), which is called instance normalization\n",
    "        - different sub-groups of output channels, which is called group normalization\n",
    "\n",
    "#### 5.3.4 Loss functions\n",
    "\n",
    "- While loss functions are traditionally applied to supervised learning tasks, where the correct label or target value tn is given for each input, it is also possible to use loss functions in an\n",
    "unsupervised setting\n",
    "\n",
    "- To train with a contrastive loss, you can run both pairs of inputs through the neural network, compute the loss, and then backpropagate the gradients through both instantiations\n",
    "(activations) of the network\n",
    "\n",
    "- It is also possible to construct a triplet loss that takes as input a pair of matching samples and a third non-matching sample and ensures that the distance between non-matching samples is greater than the distance between matches plus some margin\n",
    "\n",
    "- Weight initialization:\n",
    "    - Before we can start optimizing the weights in our network, we must first initialize them.\n",
    "\n",
    "#### 5.3.5 Backpropagation\n",
    "\n",
    "- Once we have set up our neural network by deciding on the number of layers, their widths\n",
    "and depths, added some regularization terms, defined the loss function, and initialized the\n",
    "weights, we are ready to train the network with our sample data\n",
    "    - To do this, we compute the derivatives (gradients) of the loss function En for training sample n with respect to the weights w using the chain rule\n",
    "\n",
    "    - Modern neural networks, however, may have millions of units and hence activations. The number of activations that need to be stored can be reduced by only storing them at certain layers and then re-computing the rest as needed, which goes under the name gradient checkpointing\n",
    "\n",
    "#### 5.3.6 Training and optimization\n",
    "- What we need at this point is some algorithm to turn these gradients into weight updates that will optimize the loss function and produce a network that generalizes well to new, unseen data.\n",
    "\n",
    "- stochastic gradient descent (SGD):\n",
    "    - In SGD, instead of evaluating the loss function by summing over all the training samples, we instead just evaluate a single training sample n and compute the derivatives of the associated loss\n",
    "    - In practice, the directions obtained from just a single sample are incredibly noisy estimates of a good descent direction, so the losses and gradients are usually summed over a small subset of the training data is called a minibatch\n",
    "\n",
    "- The step size parameter α is often called the learning rate and must be carefully adjusted\n",
    "to ensure good progress while avoiding overshooting and exploding gradients.\n",
    "    - In practice, it is common to start with a larger (but still small) learning rate αt and to decrease it over time so that the optimization settles into a good minimum \n",
    "\n",
    "- Regular gradient descent is prone to stalling when the current solution reaches a “flat\n",
    "spot” in the search space, and stochastic gradient descent only pays attention to the errors\n",
    "in the current minibatch. For these reasons, the SGD algorithms may use the concept of momentum\n",
    "\n",
    "- a number of more sophisticated optimization techniques have been applied to deep network training:\n",
    "    - Nesterov momentum: where the gradient is (effectively) computed at the state predicted from the velocity update\n",
    "\n",
    "    - AdaGrad (Adaptive Gradient)\n",
    "\n",
    "    - RMSProp, where the running sum of squared gradients is replaced with a leaky (decaying) sum \n",
    "\n",
    "    - Adadelta\n",
    "\n",
    "    - Adam: which combines elements of all the previous ideas into a single framework and also de-biases the initial leaky estimates\n",
    "\n",
    "    - AdamW, which is Adam with decoupled weight decay\n",
    "\n",
    "- Adam and AdamW are currently the most popular optimizers for deep networks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional neural networks\n",
    "\n",
    "- use 1 × 1 convolutions, which do not actually perform convolutions but rather combine various channels on a per-pixel basis, often with the goal of reducing the dimensionality of the feature space.\n",
    "\n",
    "- To fully determine the behavior of a convolutional layer, we still need to specify a few additional parameters. These include:\n",
    "    - Padding\n",
    "    - Stride: The default stride for convolution is 1 pixel, but it is also possible to only evaluate the convolution at every nth column and row\n",
    "    - Dilation: Extra “space” (skipped rows and column) can be inserted between pixel samples during convolution, also known as dilated\n",
    "    - Grouping: by default, all input channels are used to produce each output channel, we can also group the input and output layers into G separate groups, each of which is convolved separately, which is known as depthwise or channel separated convolution\n",
    "\n",
    "#### 5.4.1 Pooling and unpooling\n",
    "![](./images/i6.png)\n",
    "- While unpooling can be used to (approximately) reverse the effect of max pooling operation, if we want to reverse a convolutional layer, we can look at learned variants of the interpolation operator, commonly known as transposed convolution\n",
    "\n",
    "![](.//images/i7.png)\n",
    "\n",
    "- U-Nets and Feature Pyramid Networks\n",
    "\n",
    "#### 5.4.3 Network architectures\n",
    "\n",
    "- Mobile networks: MobileNet, MobileNetV2, ShuffleNet, ShuffleNetV2, ESPNet, ESPNetV2\n",
    "\n",
    "#### 5.4.4 Model zoos\n",
    "\n",
    "- [torchvision models zoo](https://github.com/pytorch/vision/tree/main/torchvision/models)\n",
    "- [PyTorch Image Models library](https://github.com/rwightman/pytorch-image-models)\n",
    "\n",
    "- Neural Architecture Search (NAS)\n",
    "    - One of the most recent trends in neural network design is the use of Neural Architecture Search (NAS) algorithms to try different network topologies and parameterizations \n",
    "    - This process is more efficient (in terms of a researcher’s time) than the trial-and-error approach that characterized earlier network design\n",
    "    - [a useful guide for training neural networks which may help avoid common issues](http://karpathy.github.io/2019/04/25/recipe)\n",
    "\n",
    "#### 5.4.5 Visualizing weights and activations\n",
    "- OpenAI also recently released a great interactive tool called [Microscope](https://microscope.openai.com/models) which allows people to visualize the significance of every neuron in many common neural networks.\n",
    "\n",
    "#### 5.4.7 Self-supervised learning\n",
    "- The idea of training on one task and then using the learning on another is called transfer\n",
    "learning, while the process of modifying the final network to its intended application and\n",
    "statistics is called domain adaptation.\n",
    "\n",
    "- The central idea in self-supervised learning is to create a supervised pretext task where\n",
    "the labels can be automatically derived from unlabeled images\n",
    "\n",
    "- Contrastive losses are a useful tool in metric learning, since they encourage distances\n",
    "in an embedding space to be small for semantically related inputs\n",
    "\n",
    "- An alternative is to use deep clustering to similarly encourage related inputs to produce similar outputs\n",
    "\n",
    "- One final variant on self-supervised learning is using a student-teacher model,  where the\n",
    "teacher network is used to provide training examples to a student network. These kinds\n",
    "of architectures were originally called model compression and knowledge distillation\n",
    "\n",
    "- VISSL provides open-source PyTorch implementations of many state-of-the-art [self-supervised learning models](https://github.com/facebookresearch/vissl) (with weights)\n",
    "\n",
    "#### 5.5 More complex models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.6 (default, Jan  8 2020, 20:23:39) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2043299c89c8cd0b4d1a6f5cf4529bd58e6a4e0fe3181a25e0d328c821cdc5c5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
