{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instance recognition\n",
    "\n",
    "- Another system that uses local affine frames: the affine region detector  is used to rectify local image patches (Figure 6.3d), from which both a SIFT descriptor and a 10 × 10 UV color histogram are computed and used for matching and recognition.\n",
    "\n",
    "#### 6.2.3 Image classification\n",
    "\n",
    "#### 6.2.4 Face Recognition\n",
    "\n",
    "![](./images/i9.png)\n",
    "\n",
    "- Facial recognition using deep learning\n",
    "\n",
    "### Object detection\n",
    "#### 6.3.1 Face detection\n",
    "- face detection techniques can be classified as feature-based, template-based, or appearance-based:\n",
    "\n",
    "    - Feature-based techniques attempt to find the locations of distinctive image features such as the eyes, nose, and mouth, and then verify whether these features are in a plausible geometrical arrangement.\n",
    "\n",
    "    - Template-based approaches, can deal with a wide range of pose and expression variability. Typically, they require good initialization near a real face and are therefore not suitable as fast face detectors.\n",
    "\n",
    "    - Appearance-based approaches scan over small overlapping rectangular patches of the image searching for likely face candidates. To deal with scale variation, the image is usually converted into a sub-octave pyramid and a separate scan is performed on each level.\n",
    "\n",
    "- quick reviews of a number of early appearance-based face detectors:\n",
    "    - Clustering and PCA\n",
    "\n",
    "    ![](./images/i10.png)\n",
    "\n",
    "    - Neural networks\n",
    "\n",
    "    - Support vector machines. \n",
    "\n",
    "    - Boosting: which involves training a series of increasingly discriminating simple classifiers and then blending their outputs \n",
    "\n",
    "    - Deep networks.\n",
    "\n",
    "#### 6.3.2 Pedestrian detection\n",
    "\n",
    "#### 6.3.3 General object detection\n",
    "\n",
    "\n",
    "- Modern object detectors: R-CNN, Fast-RCNN, Faster-RCNN, DETR, ...\n",
    "\n",
    "    ![](./images/i11.png)\n",
    "\n",
    "    - construct a Feature Pyramid Network (FPN), where top-down connections are used to endow higher-resolution (lower) pyramid levels with the semantics inferred at higher levels. This additional information significantly enhances the performance of object detectors (and other downstream tasks) and makes their behavior much less sensitive to object size.\n",
    "\n",
    "    - Two stage network: a region proposal algorithm or network selects the locations and shapes of the detections to be considered, and a second network is then used to classify and regress the pixels or features inside each region\n",
    "\n",
    "- Single-stage networks\n",
    "    - SSD, YOLO\n",
    "    - RetinaNet is also a single-stage detector built on top of a feature pyramid network\n",
    "        - It uses a focal loss to focus the training on hard examples by downweighting the loss on well-classified samples, thus preventing the larger number of easy negatives from overwhelming the training.\n",
    "\n",
    "- Open-source frameworks for training and fine-tuning object detectors include [PyTorch’s Detectron2](https://github.com/facebookresearch/detectron2)\n",
    "\n",
    "### Semantic segmentation\n",
    "- semantic segmentation (per-pixel class labeling)\n",
    "- instance segmentation (accurately delineating each separate object)\n",
    "- panoptic segmentation (labeling both objects and stuff)\n",
    "- dense pose estimation (labeling pixels belonging to people and their body parts)\n",
    "\n",
    "#### 6.4.1 Application: Medical image segmentation\n",
    "\n",
    "#### 6.4.2 Instance segmentation\n",
    "\n",
    "#### 6.4.3 Panoptic segmentation\n",
    "- semantic segmentation classifies each pixel in an image into its semantic category, i.e., what stuff does each pixel correspond to. Instance segmentation associates pixels with individual objects, i.e., how many objects are there and what are their extents\n",
    "\n",
    "-  Putting both of these systems together has long been a goal of semantic scene understanding. Doing this on a per-pixel level results in a panoptic segmentation of the scene, where all of the objects are correctly segmented and the remaining stuff is correctly labeled\n",
    "\n",
    "#### 6.4.4 Application: Intelligent photo editing\n",
    "\n",
    "#### 6.4.5 Pose estimation\n",
    "\n",
    "### Video understanding\n",
    "- It starts with the detection and description of human actions, which are taken as the basic atomic units of videos, these basic primitives can be chained into more complete descriptions of longer video sequences.\n",
    "\n",
    "![](./images/i12.png)\n",
    "\n",
    "- In the last decade, video understanding techniques have shifted to using deep networks. researchers have also investigated using optical flow as an additional input.\n",
    "\n",
    "- Finally, while action recognition is the main focus of most recent video understanding work, it is also possible to classify videos into different scene categories such as “beach”, “fireworks”, or “snowing.” This problem is called dynamic scene recognition and can be addressed using spatio-temporal CNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.6 (default, Jan  8 2020, 20:23:39) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2043299c89c8cd0b4d1a6f5cf4529bd58e6a4e0fe3181a25e0d328c821cdc5c5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
